{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "spam_df = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>641</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Message                                                            \\\n",
       "           count unique                                                top   \n",
       "Category                                                                     \n",
       "ham         4825   4516                             Sorry, I'll call later   \n",
       "spam         747    641  Please call our customer service representativ...   \n",
       "\n",
       "               \n",
       "         freq  \n",
       "Category       \n",
       "ham        30  \n",
       "spam        4  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data\n",
    "spam_df.groupby('Category').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn spam/ham into numerical values, create new column\n",
    "spam_df['spam'] = spam_df['Category'].apply(lambda x: 1 if x=='spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Split the data\n",
    "    test_size = int(len(X) * test_size)\n",
    "    train_indices = indices[:-test_size]\n",
    "    test_indices = indices[-test_size:]\n",
    "    \n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_test = y.iloc[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage:\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_df.Message, spam_df.spam, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CountVectorizer object\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary_ = {}\n",
    "        self.inverse_vocabulary_ = []\n",
    "\n",
    "    def fit(self, documents):\n",
    "        # Create a vocabulary dictionary\n",
    "        for doc in documents:\n",
    "            for word in doc.split():\n",
    "                if word not in self.vocabulary_:\n",
    "                    self.vocabulary_[word] = len(self.vocabulary_)\n",
    "                    self.inverse_vocabulary_.append(word)\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        # Create a matrix of token counts\n",
    "        rows = []\n",
    "        for doc in documents:\n",
    "            row = [0] * len(self.vocabulary_)\n",
    "            for word in doc.split():\n",
    "                if word in self.vocabulary_:\n",
    "                    row[self.vocabulary_[word]] += 1\n",
    "            rows.append(row)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "x_train_count = cv.fit_transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultinomialNB at 0x252813a1790>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with Naive Bayes\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    # Calculate the total sum of squares\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    \n",
    "    # Calculate the residual sum of squares\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Calculate R^2 score\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    return r2\n",
    "\n",
    "\n",
    "\n",
    "class MultinomialNB:\n",
    "    def __init__(self):\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate class prior probabilities\n",
    "        self.classes_, class_count = np.unique(y, return_counts=True)\n",
    "        self.class_log_prior_ = np.log(class_count / y.shape[0])\n",
    "\n",
    "        # Calculate feature probabilities\n",
    "        feature_count = np.zeros((len(self.classes_), X.shape[1]))\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            feature_count[i, :] = X[y == c].sum(axis=0)\n",
    "        \n",
    "        smoothed_fc = feature_count + 1  # Apply Laplace smoothing\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1)\n",
    "        self.feature_log_prob_ = np.log(smoothed_fc / smoothed_cc[:, np.newaxis])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return (X @ self.feature_log_prob_.T) + self.class_log_prior_\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_log_proba(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return r2_score(y, predictions)\n",
    "\n",
    "# Example usage:\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-test ham\n",
    "email_ham=[\"Hey, wanna go for a movie tonight?\"]\n",
    "email_ham_count=cv.transform(email_ham)\n",
    "model.predict(email_ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-test spam\n",
    "email_spam=[\"Free money reward !\"]\n",
    "email_spam_count=cv.transform(email_spam)\n",
    "model.predict(email_spam_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.862260646758897"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "# score is r2 score\n",
    "x_test_count=cv.transform(X_test)\n",
    "model.score(x_test_count, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
